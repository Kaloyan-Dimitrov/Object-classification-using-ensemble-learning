{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Object Classification Model Using Ensemble Learning with GrayLevel Co-Occurrence Matrix and Histogram Extraction](https://arxiv.org/ftp/arxiv/papers/2309/2309.13512.pdf)\n",
    "### Summary by Kaloyan Dimitrov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This introduction discusses the importance and challenges of object classification in various fields, including medicine, industry, and research. It highlights the difficulties posed by variations in object characteristics such as shape, size, color, texture, and context. The introduction details previous research findings, noting the effectiveness of certain methods like GLCM for feature extraction and different classification models (SVM, k-NN) in achieving high accuracy, sensitivity, specificity, and precision, particularly with Magnetic Resonance Imaging samples.\n",
    "\n",
    "The primary goal of the research is to enhance the accuracy of object identification through various classification methods:\n",
    "- K-Nearest Neighbors, \n",
    "- Random Forest, \n",
    "- Support Vector Machine, \n",
    "- Decision Tree, \n",
    "- Naive Bayes \n",
    "\n",
    "and ensemble learning techniques:\n",
    "- Voting Classifiers and\n",
    "- Combined Classifiers. \n",
    "\n",
    "The use of these combined methods is proposed to address the complexities and variations in object classification, as each model brings a unique focus on specific characteristics or features, leading to improved accuracy and reduced prediction errors.\n",
    "\n",
    "Additionally, the research utilizes GLCM (Gray-Level Co-Occurrence Matrix) for feature extraction and histograms to identify relevant characteristics from images. These methods are effective in describing spatial relationships between pixel intensities and the frequency of pixel intensity levels. By integrating ensemble learning with GLCM feature extraction and histograms, the study aims to create a more accurate object classification model, enhancing the effectiveness of ensemble learning methods and increasing reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methods\n",
    "### 1. Pre-Processing and Resizing of Object Datasets\n",
    "The first step involves preparing the images. This includes resizing the images in the dataset to a uniform size. This step is crucial because it makes the next steps - feature extraction and classification - easier and more consistent.\n",
    "### 2. Feature Extraction Using GLCM and Histograms\n",
    "- **GLCM *(Gray-Level Co-occurrence Matrix)***: This is a method that looks at how often pairs of pixels with specific values and in a specified spatial relationship occur in an image. This method helps to extract various features from the image, like texture and contrast.\n",
    "- **Histograms**: These are used to analyze the distribution of pixels in an image. Basically, it's like looking at a graph showing how many pixels of each intensity level are present in the image.\n",
    "\n",
    "Below is the code for the histogram calculation, which also describes the process in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateHistogram(image):\n",
    "\t\"\"\"\n",
    "\tCalculates the histogram of the given image.\n",
    "\t\n",
    "\tParameters\n",
    "\t----------\n",
    "\timage : numpy.ndarray\n",
    "\t\tThe image to calculate the histogram of.\n",
    "\t\n",
    " \tReturns\n",
    "\t-------\n",
    "\tnumpy.ndarray\n",
    "\t\tThe calculated histogram.\n",
    "\t\n",
    "\tNotes\n",
    "\t-----\n",
    "\t* The image is assumed to be grayscale. For color images, the image should be converted to grayscale before passing it to this function.\n",
    "\t* The histogram is calculated for intensity levels 0 to 255.\n",
    "\t\"\"\"\n",
    "\t# Step 1: Determine the number of possible intensity levels in the image.\n",
    "\t# Assuming the image is grayscale, the number of intensity levels is typically 256 (0 to 255).\n",
    "\tN = 256\n",
    "\n",
    "\t# Step 2: Create an array called histogram with size N, initialized with zeros.\n",
    "\thistogram = np.zeros(N, dtype=int)\n",
    "\n",
    "\t# Step 3: Get the width and height of the image.\n",
    "\theight, width = image.shape\n",
    "\n",
    "\t# Step 4: Iterate over each pixel in the image using two nested loops.\n",
    "\tfor y in range(height):\n",
    "\t\tfor x in range(width):\n",
    "\t\t\t# Step 5: Retrieve the intensity of the pixel at coordinate (x, y) in the image.\n",
    "\t\t\tintensity = image[y, x]\n",
    "\n",
    "\t\t\t# Step 6: Increment the corresponding element in the histogram array for the found intensity level.\n",
    "\t\t\thistogram[intensity] += 1\n",
    "\n",
    "\t# Step 7: Repeat steps 4 and 5 for each pixel in the image (done within the nested loops).\n",
    "\n",
    "\t# Step 8: Return the calculated histogram.\n",
    "\treturn histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to this, the process involves calculating specific features from the GLCM (Grey Level Co-occurrence Matrix). But to do that we first have to calculate the actual GLCM.\n",
    "\n",
    "The GLCM is a matrix where the number of rows and columns is equal to the number of grey levels, G, in the image. The matrix element P(i, j | d, θ) is the relative frequency with which two pixels, separated by a pixel distance d and direction θ, occur within a given neighborhood, one with intensity 'i' and the other with intensity 'j'. The elements of GLCM are, therefore, the probabilities of co-occurrence of pixel pairs with specific values and at a specific spatial relationship.\n",
    "\n",
    "To calculate the co-occurrence probabilities, we would:\n",
    "\n",
    "1. Define a spatial relationship (distance and direction) between pixel pairs.\n",
    "2. For each pixel in the image, find the pixel that is 'd' distance away in the 'θ' direction.\n",
    "3. If the intensity of the first pixel is 'i' and the intensity of the second pixel is 'j', increment the count in the (i, j) cell of the GLCM.\n",
    "4. Normalize the GLCM by dividing each entry by the total number of pixel pairs considered. The result is the co-occurrence probabilities.\n",
    "\n",
    "This matrix provides information about the texture of the image, which can be used for tasks like image segmentation, object detection, and more. The code below shows how to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_greycomatrix(image):\n",
    "\t\"\"\"\n",
    "\tCalculate a simple GLCM for an image with a distance of 1 and a 0-degree angle.\n",
    "\n",
    "\tParameters:\n",
    "\timage (numpy.ndarray): Input image\n",
    "\n",
    "\tReturns:\n",
    "\tnumpy.ndarray: The GLCM\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Initialize the GLCM\n",
    "\tglcm = np.zeros((256, 256))\n",
    "\n",
    "\t# Shift the image one pixel to the right to get the neighbors\n",
    "\timage_shifted = np.roll(image, shift=-1, axis=1)\n",
    "\n",
    "\t# Calculate the GLCM\n",
    "\tfor i in range(image.shape[0]):\n",
    "\t\tfor j in range(image.shape[1] - 1):  # Subtract 1 to avoid the last column\n",
    "\t\t\tintensity = int(image[i, j])\n",
    "\t\t\tneighbor_intensity = int(image_shifted[i, j])\n",
    "\t\t\tglcm[intensity, neighbor_intensity] += 1\n",
    "\n",
    "\treturn glcm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having calculated this matrix we can now extract specific features from it:\n",
    "- Energy (Equation 1): This measures the uniformity of texture in the image. It's calculated by summing up the squares of the co-occurrence probabilities for each pair of pixels.\n",
    "$$ Energy = \\sum_{i,j} P(i,j)^2 $$\n",
    "- Contrast (Equation 2): This measures the variation between neighboring pixels. It’s calculated by finding the difference between the row and column indices, squaring this difference, and multiplying it by the co-occurrence - probability.\n",
    "$$ Contrast = \\sum_{i,j} (i-j)^2 P(i,j) $$\n",
    "- Homogeneity (Equation 3): This measures how similar the elements in the co-occurrence matrix are, in terms of proximity.\n",
    "$$ Homogeneity = \\sum_{i,j} \\frac{P(i,j)}{1+(i-j)^2} $$\n",
    "- Entropy (Equation 4): This one is about the complexity of the information in the image. It’s calculated using the co-occurrence probabilities of pixel pairs.\n",
    "$$ Entropy = -\\sum_{i,j} P(i,j) log_2 P(i,j) $$\n",
    "- Correlation (Equation 5): This measures the linear dependency between pixel intensities. It involves the average pixel intensity and the standard deviation of the row and column weights of the co-occurrence matrix.\n",
    "$$ Correlation = \\frac{\\sum_{i,j} [ij * P(i,j)] - \\mu_x * \\mu_y}{\\sigma_x * \\sigma_y} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_glcm_features(glcm):\n",
    "\t\"\"\"\n",
    "\tCalculate GLCM features for a given GLCM\n",
    "\t\n",
    "\tParameters:\n",
    "\tglcm (numpy.ndarray): Input GLCM\n",
    "\t\n",
    "\tReturns:\n",
    "\tdict: A dictionary with GLCM features\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef energy(glcm):\n",
    "\t\t\"\"\"Calculate energy feature\"\"\"\n",
    "\t\treturn np.sum(glcm**2)\n",
    "\t\n",
    "\tdef contrast(glcm):\n",
    "\t\t\"\"\"Calculate contrast feature\"\"\"\n",
    "\t\trows, cols = np.indices(glcm.shape)\n",
    "\t\treturn np.sum(glcm * (rows - cols)**2)\n",
    "\t\n",
    "\tdef homogeneity(glcm):\n",
    "\t\t\"\"\"Calculate homogeneity feature\"\"\"\n",
    "\t\trows, cols = np.indices(glcm.shape)\n",
    "\t\treturn np.sum(glcm / (1 + np.abs(rows - cols)))\n",
    "\t\n",
    "\tdef entropy(glcm):\n",
    "\t\t\"\"\"Calculate entropy feature\"\"\"\n",
    "\t\tglcm_prob = glcm / np.sum(glcm)  # Convert GLCM to probabilities\n",
    "\t\tglcm_prob_nonzero = glcm_prob[glcm_prob > 0]  # Only consider non-zero entries\n",
    "\t\treturn -np.sum(glcm_prob_nonzero * np.log2(glcm_prob_nonzero))  # Calculate entropy\n",
    "\t\n",
    "\tdef correlation(glcm):\n",
    "\t\t\"\"\"Calculate correlation feature\"\"\"\n",
    "\t\trows, cols = np.indices(glcm.shape)\n",
    "\t\tmu_x = np.sum(rows * glcm)\n",
    "\t\tmu_y = np.sum(cols * glcm)\n",
    "\t\tsigma_x = np.sqrt(np.sum(glcm * (rows - mu_x)**2))\n",
    "\t\tsigma_y = np.sqrt(np.sum(glcm * (cols - mu_y)**2))\n",
    "\t\treturn np.sum(((rows - mu_x) * (cols - mu_y) * glcm) / (sigma_x * sigma_y))\n",
    "\t\n",
    "\t# Calculate GLCM properties\n",
    "\tfeatures = {\n",
    "\t\t'energy': energy(glcm),\n",
    "\t\t'contrast': contrast(glcm),\n",
    "\t\t'homogeneity': homogeneity(glcm),\n",
    "\t\t'entropy': entropy(glcm),\n",
    "\t\t'correlation': correlation(glcm)\n",
    "\t}\n",
    "\t\n",
    "\treturn features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classification Methods\n",
    "The features extracted are then classified using several algorithms:\n",
    "- **Random Forest (RF)**: This method uses many decision trees to make a decision.\n",
    "- **Support Vector Machine (SVM)**: This method finds the best boundary that separates different classes of objects in the images.\n",
    "- **k-Nearest Neighbors (kNN)**: This one looks at the nearest neighbors of a data point to decide its class.\n",
    "- **Naive Bayes**: This method works on the principle that the presence of one feature doesn't affect the presence of another.\n",
    "- **Decision Tree**: It's like a flowchart where each decision leads down a different path to a classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ensemble Methods - Voting and Combined Classifier:\n",
    "Using the above described classification methods, the research then uses ensemble learning to combine the predictions of the different models. This is done in two separate ways:\n",
    "- **Voting Ensemble**: This method combines predictions from different models and goes with the majority vote.\n",
    "- **Combined Classifier**: This is more dynamic. If one model is unsure about a prediction, it will rely on the predictions from other models.\n",
    "\n",
    "Here is also their implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def VotingEnsemble(RF_predict, SVM_predict, kNN_predict, NB_predict, DT_predict):\n",
    "\t\"\"\"\n",
    "\tPerforms voting ensemble on the predictions from different models.\n",
    "\t\n",
    "\tParameters:\n",
    "\tRF_predict (list): Predictions from the Random Forest model.\n",
    "\tSVM_predict (list): Predictions from the Support Vector Machine model.\n",
    "\tkNN_predict (list): Predictions from the k-Nearest Neighbors model.\n",
    "\tNB_predict (list): Predictions from the Naive Bayes model.\n",
    "\tDT_predict (list): Predictions from the Decision Tree model.\n",
    "\t\n",
    "\tReturns:\n",
    "\tlist: Ensemble predictions.\n",
    "\t\"\"\"\n",
    "\tensemble_predict = []\n",
    "\t\n",
    "\t# Iterate over the predictions\n",
    "\tfor i in range(len(RF_predict)):\n",
    "\t\tpredict_list = [RF_predict[i], SVM_predict[i], kNN_predict[i], NB_predict[i], DT_predict[i]]\n",
    "\t\t\n",
    "\t\t# Perform majority voting\n",
    "\t\tvote = Counter(predict_list).most_common(1)[0][0]\n",
    "\t\t\n",
    "\t\t# Append the ensemble prediction\n",
    "\t\tensemble_predict.append(vote)\n",
    "\t\n",
    "\treturn ensemble_predict\n",
    "\n",
    "\n",
    "def CombinedEnsemble(RF_predict, SVM_predict, kNN_predict, NB_predict, DT_predict):\n",
    "\t\"\"\"\n",
    "\tPerforms combined ensemble on the predictions from different models.\n",
    "\t\n",
    "\tParameters:\n",
    "\tRF_predict (list): Predictions from the Random Forest model.\n",
    "\tSVM_predict (list): Predictions from the Support Vector Machine model.\n",
    "\tkNN_predict (list): Predictions from the k-Nearest Neighbors model.\n",
    "\tNB_predict (list): Predictions from the Naive Bayes model.\n",
    "\tDT_predict (list): Predictions from the Decision Tree model.\n",
    "\t\n",
    "\tReturns:\n",
    "\tlist: Ensemble predictions.\n",
    "\t\"\"\"\n",
    "\tensemble_predict = []\n",
    "\t\n",
    "\t# Iterate over the predictions\n",
    "\tfor i in range(len(RF_predict)):\n",
    "\t\t# Check the predictions in order of priority\n",
    "\t\tif RF_predict[i] != \"unknown\":\n",
    "\t\t\tensemble_predict.append(RF_predict[i])\n",
    "\t\telif SVM_predict[i] != \"unknown\":\n",
    "\t\t\tensemble_predict.append(SVM_predict[i])\n",
    "\t\telif kNN_predict[i] != \"unknown\":\n",
    "\t\t\tensemble_predict.append(kNN_predict[i])\n",
    "\t\telif NB_predict[i] != \"unknown\":\n",
    "\t\t\tensemble_predict.append(NB_predict[i])\n",
    "\t\telse:\n",
    "\t\t\tensemble_predict.append(DT_predict[i])\n",
    "\t\n",
    "\treturn ensemble_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole process is summarized in the following diagram:\n",
    "\n",
    "![classification_model_flowchart.png](./classification_model_flowchart.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results and Discussion\n",
    "First let's implement the methods described above and see how they perform on a benchmark dataset. We will use the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset, which contains 60,000 images of 10 different classes. The images are 32x32 pixels and are colored. The classes are: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/69687794/unable-to-manually-load-cifar10-dataset\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "train_images_gray = rgb2gray(train_images)\n",
    "test_images_gray = rgb2gray(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaloy\\AppData\\Local\\Temp\\ipykernel_41496\\939482465.py:39: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.sum(((rows - mu_x) * (cols - mu_y) * glcm) / (sigma_x * sigma_y))\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to hold the GLCM features for each image\n",
    "train_features = []\n",
    "test_features = []\n",
    "\n",
    "# Calculate the GLCM features for each image in the training set\n",
    "for image in train_images_gray:\n",
    "\tglcm = simple_greycomatrix(image)\n",
    "\tfeatures = calculate_glcm_features(glcm)\n",
    "\ttrain_features.append(features)\n",
    "\n",
    "# Calculate the GLCM features for each image in the test set\n",
    "for image in test_images_gray:\n",
    "\tglcm = simple_greycomatrix(image)\n",
    "\tfeatures = calculate_glcm_features(glcm)\n",
    "\ttest_features.append(features)\n",
    "\n",
    "# Now, train_features and test_features are lists of dictionaries containing the GLCM features for each image.\n",
    "# We can use these features to train and evaluate your machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train_features to a numerical array\n",
    "train_features_array = np.array([list(features.values()) for features in train_features])\n",
    "\n",
    "# Train each model using training data\n",
    "rf = RandomForestClassifier().fit(train_features_array, train_labels)\n",
    "svm = SVC(probability=True).fit(train_features_array, train_labels)\n",
    "knn = KNeighborsClassifier().fit(train_features_array, train_labels)\n",
    "nb = GaussianNB().fit(train_features_array, train_labels)\n",
    "dt = DecisionTreeClassifier().fit(train_features_array, train_labels)\n",
    "\n",
    "# Convert test_features to a numerical array\n",
    "test_features_array = np.array([list(features.values()) for features in test_features])\n",
    "\n",
    "# Predict with each model using test data\n",
    "RF_predict = rf.predict(test_features_array)\n",
    "SVM_predict = svm.predict(test_features_array)\n",
    "kNN_predict = knn.predict(test_features_array)\n",
    "NB_predict = nb.predict(test_features_array)\n",
    "DT_predict = dt.predict(test_features_array)\n",
    "DT_predict = dt.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_results = VotingEnsemble(RF_predict, SVM_predict, kNN_predict, NB_predict, DT_predict)\n",
    "combined_results = CombinedEnsemble(RF_predict, SVM_predict, kNN_predict, NB_predict, DT_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately I didn't have time to actually test the code, but here are the summarized results from the original article:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Performance of Individual Models:**\n",
    "- **Random Forest (RF)**: Exhibited exceptional performance with 99.09% accuracy, 99.28% precision, and 98.96% recall, showing a strong balance in minimizing classification errors.\n",
    "- **K-Nearest Neighbors (KNN) and Decision Tree:** Demonstrated average performance with 76.13% and 79.73% accuracy, respectively. Both models maintained a balance between precision and recall, indicating relatively balanced error - rates for positive and negative classifications.\n",
    "- **Support Vector Machine (SVM)**: Showed the lowest performance with only 43.47% accuracy, 43.52% precision, and 41.48% recall, indicating frequent misclassifications.\n",
    "- **Naive Bayes (NB)**: Had an accuracy of 50.90%, with 56.55% precision but a lower recall of 46.09%, indicating challenges in correctly identifying positive classes.\n",
    "\n",
    "2. **Ensemble Models:**\n",
    "- **Combined Classifier**: Achieved an accuracy of 98.88%, 99.01% precision, 98.72% recall, and an F1-score of 98.86%, indicating superior performance in classification.\n",
    "- **Voting Ensemble**: Recorded lower metrics with 87.39% accuracy, 88.42% precision, 86.24% recall, and an F1 score of 86.96%.\n",
    "\n",
    "3. **Confusion Matrix Analysis**\n",
    "- **Voting Ensemble**: Performed well in predicting Classes 0, 2, and 3, but had some difficulty with Class 2.\n",
    "- **Combined Classifier**: Showed almost identical high accuracy in all classes to the Voting Ensemble, with perfect predictions for classes 1 and 3.\n",
    "\n",
    "4. **Bar Chart Analysis** (Fig. 4):\n",
    "\n",
    "![fig4.png](./fig4.png)\n",
    "\n",
    "- Models like RF, Voting Ensemble, and Combined Classifier nearly reached 100% accuracy in predicting all classes.\n",
    "- SVM and NB were less accurate, particularly in predicting classes 1, 2, and 3.\n",
    "\n",
    "5. **Overall Model Evaluation** (Table 3):\n",
    "\n",
    "| Classifier | Accuracy | Precision | Recall | F1 Score |\n",
    "|------------|----------|-----------|--------|----------|\n",
    "| Random Forest (RF) | 0.993 | 0.976 | 1.000 | 0.988 |\n",
    "| k-Nearest Neighbors (k-NN) | 0.871 | 0.730 | 0.852 | 0.786 |\n",
    "| Decision Tree (Tree) | 0.868 | 0.778 | 0.790 | 0.784 |\n",
    "| Support Vector Machine (SVM) | 0.599 | 0.302 | 0.481 | 0.371 |\n",
    "| Naive Bayes (NB) | 0.665 | 0.198 | 0.658 | 0.305 |\n",
    "| Voting Ensemble (VE) | 0.924 | 0.786 | 0.952 | 0.861 |\n",
    "| Combined Classifier (CC) | 0.993 | 0.976 | 1.000 | 0.988 |\n",
    "\n",
    "- RF and Combined Classifier had the highest accuracy (0.993), precision (0.976), recall (1.000), and F1 score (0.988).\n",
    "- SVM had the lowest accuracy (0.599) and recall (0.481), while NB had the lowest precision (0.198)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "The conclusion of the study highlights the performance of various classification models and their use in ensemble methods:\n",
    "\n",
    "1. **Performance of Individual Models**:\n",
    "\n",
    "- **Random Forest (RF)**: Exhibited high accuracy, standing out as the most effective model among those tested.\n",
    "- **Support Vector Machine (SVM) and Naive Bayes (NB):** Struggled with classification tasks, with SVM notably recording the lowest accuracy at 43.47%.\n",
    "- **K-Nearest Neighbors (KNN) and Decision Tree**: Demonstrated moderate performance but maintained a balance between precision and recall.\n",
    "2. **Ensemble Models**:\n",
    "\n",
    "- Voting Ensemble: Achieved an accuracy of 87.39%, showing good potential.\n",
    "- Combined Classifier: Outperformed the Voting Ensemble with an accuracy of 98.88%, precision of 99.01%, recall of 98.72%, and an F1 score of 98.86%.\n",
    "3. **Implications for Future Development**:\n",
    "- The success of the Combined Classifier, in particular, suggests that ensemble methods can significantly enhance the performance of models with lower accuracy. This opens avenues for further development and improvement in classification tasks.\n",
    "\n",
    "In summary, the study demonstrates the effectiveness of ensemble methods, especially the Combined Classifier, in elevating the performance of individual classification models. This approach is especially beneficial for models that individually show lower accuracy, highlighting the potential of ensemble techniques in the field of classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teeth_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
